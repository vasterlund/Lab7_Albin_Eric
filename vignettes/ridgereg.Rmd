---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r,echo=FALSE, message=FALSE}
library(mlbench)
library(caret)
library(Lab7SpaghettiBolognese)
data("BostonHousing")
```


## 1)

We will devide the `BostonHousing` dataset in to a training and a test set.

```{r}
set.seed(12345)
training <- createDataPartition(BostonHousing$tax,p = 0.7)

train_data <- BostonHousing[training$Resample1, ]
test_data <- BostonHousing[-training$Resample1, ]
```




## 2)

This is our full linear model with the respons variable `tax`.

```{r}
model_lm <- train(tax ~ .  ,data = train_data, method = "lm")
sol <- summary(model_lm)
sol
```

A lot of parameters is un-significant. So we need to reduce our model. We do it by forward selection.


```{r}
model_leap_forward <- train(tax ~ .  ,data = train_data, method = "leapForward")

summary_model_leap_forward <-summary(model_leap_forward)
summary_model_leap_forward$which

model_lm <- train(tax ~ zn + indus + rad + medv  ,data = train_data, method = "lm")
summary(model_lm)


```

We can see that the model choosen contains the expnatory variables: `zn`, `indus`, `rad` and `medv`. 



## 3)

Now we want to evaluate the model and compare it with the other model in the forward selection.

```{r}
model_leap_forward

```

The final model have smalest RMSE, and highest adjusted r-square which make the model the best one. 




## 4)


```{r}
ridgereg$new(tax ~ zn + indus + rad + medv, data=train_data ,lambda=seq(0.1,0.5,0.1))$print()
```










