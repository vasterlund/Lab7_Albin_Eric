---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r,echo=FALSE, message=FALSE}
library(mlbench)
library(caret)
library(Lab7SpaghettiBolognese)
data("BostonHousing")
```


## 1)

We will devide the `BostonHousing` dataset in to a training and a test set.

```{r}
set.seed(12345)

training <- createDataPartition(BostonHousing$tax,p = 0.7)

train_data <- BostonHousing[training$Resample1, ]
test_data <- BostonHousing[-training$Resample1, ]
```




## 2)

This is our full linear model with the respons variable `tax`.

```{r}
model_lm <- train(tax ~ .  ,data = train_data, method = "lm")
sol <- summary(model_lm)
sol
```

A lot of parameters is un-significant. So we need to reduce our model. We do it by forward selection.


```{r}
model_leap_forward <- train(tax ~ .  ,data = train_data, method = "leapForward")

summary_model_leap_forward <-summary(model_leap_forward)
summary_model_leap_forward$which

model_lm <- train(tax ~ zn + indus + rad + medv  ,data = train_data, method = "lm")
summary(model_lm)


```

We can see that the model choosen contains the expnatory variables: `zn`, `indus`, `rad` and `medv`. 



## 3)

Now we want to evaluate the model and compare it with the other model in the forward selection.

```{r}
model_leap_forward

```

The final model have smalest RMSE, and highest adjusted r-square which make the model the best one. 




## 4)

In this task we will test different $\lambda$ for our `ridereg()` function. We had to do a function with our train for our vingnette to actually work.

```{r}
# List of our own model
ridgereg_train(lambda = c(3,4,6,8,5), normalize = FALSE)

```

We test for the values $\lambda = 3,4,6,8, 5$ and we get that the best lambda is 8 when looking at the `RMSE`. 



## 5)

We will do a 10-fold cross-validation in the training data set. 


```{r}

ridgereg_train(lambda = c(0,3,4,6,8,5),cross_val = TRUE,normalize = FALSE)

```


We perform the cross-validation 10 times for $\lambda = 3,4,6,8, 5$. We set the normalize to `FALSE` do we can compare the `RMSE` values. 


## 6)


```{r, echo=FALSE}

#ridgereg_train(lambda = c(0,3,4,6,8,5),cross_val = TRUE, normalize = FALSE)


```











