---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r,echo=FALSE, message=FALSE}
library(mlbench)
library(caret)
library(Lab7SpaghettiBolognese)
data("BostonHousing")
```


## 1)

We will devide the `BostonHousing` dataset in to a training and a test set.

```{r}
set.seed(12345)
training <- createDataPartition(BostonHousing$tax,p = 0.7)

train_data <- BostonHousing[training$Resample1, ]
test_data <- BostonHousing[-training$Resample1, ]
```




## 2)

This is our full linear model with the respons variable `tax`.

```{r}
model_lm <- train(tax ~ .  ,data = train_data, method = "lm")
sol <- summary(model_lm)
sol
```

A lot of parameters is un-significant. So we need to reduce our model. We do it by forward selection.


```{r}
model_leap_forward <- train(tax ~ .  ,data = train_data, method = "leapForward")

summary_model_leap_forward <-summary(model_leap_forward)
summary_model_leap_forward$which

model_lm <- train(tax ~ zn + indus + rad + medv  ,data = train_data, method = "lm")
summary(model_lm)


```

We can see that the model choosen contains the expnatory variables: `zn`, `indus`, `rad` and `medv`. 



## 3)

Now we want to evaluate the model and compare it with the other model in the forward selection.

```{r}
model_leap_forward

```

The final model have smalest RMSE, and highest adjusted r-square which make the model the best one. 




## 4)

In this task we will 

```{r}
# List of our own model
ridgereg_train<-function(lambda=0){
  
  ridgeregg  <- list(type = "Regression", 
                     library = "Lab7SpaghettiBolognese",
                     loop = NULL,
                     prob = NULL)
  
  ridgeregg$parameters <- data.frame(parameter = "lambda",
                                     class = "numeric",
                                     label = "Ridge Regression")
  
  
  ridgeregg$grid <- function (x, y, len = NULL, search = "grid"){
    data.frame(lambda = lambda)
  } 
  
  ridgeregg$fit <- function (x, y, wts, param, lev, last, classProbs, ...) {
    dat <- if (is.data.frame(x)) 
      x
    else as.data.frame(x)
    dat$.outcome <- y
    out <- ridgereg$new(.outcome ~ ., data=dat ,lambda = param$lambda, ...)
    
    out
  }
  
  ridgeregg$predict <- function (modelFit, newdata, submodels = NULL) {
    if (!is.data.frame(newdata)) 
      newdata <- as.data.frame(newdata)
    modelFit$predict(newdata)
  }
  
  
  train(tax ~ zn + indus + rad + medv  ,data = train_data, method = ridgeregg)
  
}


```


```{r}


ridgereg_train(0.5)


```


## 5)















